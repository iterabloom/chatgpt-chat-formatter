{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-TcKOsXUAKHY"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def parse_text_file(file_path):\n",
        "    list_of_tuples = []\n",
        "\n",
        "    with open(file_path, 'r') as file:\n",
        "        for line in file:\n",
        "            parts = line.split(\" | \", 1)  # Split the line into two parts: line number + code, and content\n",
        "            if len(parts) == 2:\n",
        "                number, code = parts[0].strip().split()\n",
        "                line_number = int(number)\n",
        "                content = parts[1].rstrip()\n",
        "                list_of_tuples.append((line_number, code, content))\n",
        "\n",
        "    return list_of_tuples"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def super_clean_text(input_string):\n",
        "    # Convert the string to lowercase\n",
        "    lowercased_string = input_string.lower()\n",
        "\n",
        "    # Remove all characters that are not a-z or 0-9 using regex\n",
        "    cleaned_string = re.sub(r'[^a-z0-9]', '', lowercased_string)\n",
        "\n",
        "    return cleaned_string\n",
        "\n",
        "\n",
        "def get_index_of_sentence(s, text):\n",
        "    lines = [line.strip() for line in text.splitlines() if line.strip()]\n",
        "    lines_clean = []\n",
        "    for l in lines:\n",
        "        c = super_clean_text(l)\n",
        "        lines_clean.append(c)\n",
        "    median = len(s)//2\n",
        "    quartile_size = len(s)//4\n",
        "    delta = max(quartile_size//2,1)\n",
        "    chunks = (\n",
        "        s[max(0,median-quartile_size-(2*delta)):median+quartile_size-(2*delta)],\n",
        "        s[median-quartile_size-(1*delta):median+quartile_size-(1*delta)],\n",
        "        s[median-quartile_size:median+quartile_size],\n",
        "        s[median-quartile_size+(1*delta):median+quartile_size+(1*delta)],\n",
        "        s[median-quartile_size+(2*delta):median+quartile_size+(2*delta)],\n",
        "    )\n",
        "    for chunk in chunks:\n",
        "        s_clean = super_clean_text(chunk)\n",
        "        #print(f\"s_clean: {s_clean}\")\n",
        "        for i, lc in enumerate(lines_clean):\n",
        "            #print(f\"lc: {lc}\")\n",
        "            if s_clean in lc:\n",
        "                #print(f\"i: {i}\")\n",
        "                return i\n",
        "    return -1"
      ],
      "metadata": {
        "id": "6sJA_XBh54gb"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_text_into_sentences(text, line_numbers, codes):\n",
        "    original_text = text\n",
        "    # Remove extra tabs\n",
        "    text = re.sub(r'\\t+', ' ', text)\n",
        "\n",
        "    # Remove all asterisks and pound signs\n",
        "    text = re.sub(r'[\\*#]', '', text)\n",
        "\n",
        "    # Replace multiple newlines with a placeholder to identify paragraphs\n",
        "    text = re.sub(r'\\n{2,}', r'###PARAGRAPH###', text)\n",
        "\n",
        "    # Replace single newlines with spaces\n",
        "    text = re.sub(r'\\n+', ' ', text)\n",
        "\n",
        "    # Remove consecutive spaces\n",
        "    text = re.sub(r'\\s{2,}', ' ', text)\n",
        "\n",
        "    # Special handling for \"v.\" in legal citations and other exceptions\n",
        "    text = re.sub(r'\\b(v\\.)\\s+', r'\\1###NO_SPLIT###', text)\n",
        "\n",
        "    # Protect abbreviations, titles, section names, numbering like \"1\\.\", and Roman numerals like \"II.\", \"Pp.\"\n",
        "    text = re.sub(r'\\b(Dr|Mr|Ms|Mrs|U\\.S|Jr|Sr|Cf|cf|art|Art|Pp|pp|[0-9]+|I{1,3}|IV|V|VI|VII|VIII|IX|X|XI|XII|Ibid)\\.\\s+', r'\\1.###NO_SPLIT###', text)\n",
        "\n",
        "    # Protect single-letter section names like \"A.\", \"B.\"\n",
        "    text = re.sub(r'\\b([A-Z])\\.\\s+', r'\\1###NO_SPLIT###', text)\n",
        "\n",
        "    # Protect section numbers like \"(1)\", \"(2)\", \"(a)\"\n",
        "    text = re.sub(r'\\((\\d+|[a-zA-Z])\\)\\s+', r'(\\1)###NO_SPLIT###', text)\n",
        "\n",
        "    def split_after_pattern(text, pattern):\n",
        "        # Use re.split but include the pattern in the split\n",
        "        parts = re.split(pattern, text)\n",
        "\n",
        "        # Reassemble the parts to ensure each matched pattern is followed by its text\n",
        "        sentences = []\n",
        "        for i in range(1, len(parts)):  # Iterate over pattern matches\n",
        "            sentences.append(parts[i].strip())\n",
        "\n",
        "        if len(parts)==1:\n",
        "            sentences = [text]\n",
        "        #elif len(parts) > 1:\n",
        "        #    print(f\"[split_after_pattern] sentences: {sentences}\")\n",
        "\n",
        "        return sentences\n",
        "\n",
        "    # Adjusted regex pattern to handle sentence splitting, especially around footnotes\n",
        "    sentence_endings = re.compile(\n",
        "        r'(?<!###NO_SPLIT###)\\.\\s+(?=[A-Z])|'   # Split at normal sentence boundaries\n",
        "        r'(?<=\\.\\\")\\s+(?=[A-Z])|'               # Period followed by quote and space\n",
        "        r'(?<=[!?])\\s+(?=[A-Z])|'              # Exclamation or question mark followed by space\n",
        "        r'(?<=\\))\\s+(?=[A-Z])|'                 # Closing parenthesis followed by space\n",
        "        r'(?<=\\])\\s+(?=[A-Z])|'                 # Footnote marker followed by space, but only if followed by an uppercase letter (new sentence)\n",
        "        r'(?<=\\.\\s)(?=\\(?[A-Z])|'              # Handling for \"(iv)\", \"(ii)\", etc.\n",
        "        r'(?<=\\.\\s)(?=[A-Z]\\w+ v\\.\\s[A-Z])|'   # Handling for \"Nixon v. Fitzgerald\"\n",
        "        r'(?=\\[\\d{4}-\\d{2}-\\d{2})|'             # Split before a timestamp pattern \"[2024-07-23...\"\n",
        "        r'(?<=\\.\\s)(?=Pp\\.\\s[0-9]+)'           # Handling for \"Pp. 24-28\"\n",
        "    )\n",
        "\n",
        "    # Split the text into sentences\n",
        "    sentences = sentence_endings.split(text.strip())\n",
        "\n",
        "    # Apply split_after_pattern to each sentence and flatten the result\n",
        "    # Pattern to match the entire timestamp, descriptor, and model line\n",
        "    # split after timestamp metadata of assistant or tool message\n",
        "    pattern = r'(\\[\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2} UTC\\] \\([A-Za-z]+\\) - Model: [^\\:]{5,6}: )'\n",
        "    temp_sentences = []\n",
        "    for s in sentences:\n",
        "        temp_sentences.extend(split_after_pattern(s, pattern))\n",
        "    sentences = temp_sentences\n",
        "\n",
        "    # split after timestamp metadata of user message\n",
        "    pattern = r'(\\[\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2} UTC\\] \\(User\\): )'\n",
        "    temp_sentences = []\n",
        "    for s in sentences:\n",
        "        temp_sentences.extend(split_after_pattern(s, pattern))\n",
        "    sentences = temp_sentences\n",
        "\n",
        "    # Replace placeholder back to original\n",
        "    sentences = [s.replace(\"###NO_SPLIT###\", \" \") for s in sentences]\n",
        "\n",
        "    # Correctly merge footnote markers back into the sentence when inside quotes or before dashes\n",
        "    merged_sentences = []\n",
        "    temp_sentence = \"\"\n",
        "    for sentence in sentences:\n",
        "        if re.match(r'^\\[\\^\\d+\\]$', sentence):\n",
        "            temp_sentence += sentence  # Append footnote marker to previous sentence\n",
        "        else:\n",
        "            if temp_sentence:\n",
        "                merged_sentences.append(temp_sentence.strip())\n",
        "                temp_sentence = \"\"\n",
        "            merged_sentences.append(sentence.strip())\n",
        "\n",
        "    # Ensure any remaining temp_sentence is added\n",
        "    if temp_sentence:\n",
        "        merged_sentences.append(temp_sentence.strip())\n",
        "\n",
        "    # Split based on paragraph boundaries\n",
        "    final_sentences_with_paragraphs = []\n",
        "    for sentence in merged_sentences:\n",
        "        final_sentences_with_paragraphs.extend(sentence.split('###PARAGRAPH###'))\n",
        "\n",
        "    # Remove the unnecessary colons\n",
        "    final_sentences_with_paragraphs = [re.sub(r'^: ', '', s).strip() for s in final_sentences_with_paragraphs]\n",
        "\n",
        "    # Return the cleaned list of sentences\n",
        "    cleaned_sentences = [sentence.strip() for sentence in final_sentences_with_paragraphs if sentence.strip()]\n",
        "\n",
        "    line_number_for_each_sentence, code_for_each_sentence = [], []\n",
        "    for s in cleaned_sentences:\n",
        "        i = get_index_of_sentence(s, original_text)\n",
        "        line_number_for_each_sentence.append(line_numbers[i])\n",
        "        code_for_each_sentence.append(codes[i])\n",
        "\n",
        "    return cleaned_sentences, line_number_for_each_sentence, code_for_each_sentence"
      ],
      "metadata": {
        "id": "jTE8I80EGKlT"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sliding_window(list_of_tuples, n):\n",
        "    whole_trajectory = []\n",
        "    extra_buffer_for_whole_trajectory = []\n",
        "    for i in range(len(list_of_tuples) - n + 1):\n",
        "        cumulative_line_numbers = tuple()\n",
        "        cumulative_codes = tuple()\n",
        "        cumulative_content = \"\"\n",
        "        window_group = []\n",
        "        for j in range(n):\n",
        "            cumulative_line_numbers += (list_of_tuples[i + j][0],)\n",
        "            cumulative_codes += (list_of_tuples[i + j][1],)\n",
        "            cumulative_content += list_of_tuples[i + j][2] + \"\\n\"\n",
        "            window_group.append((cumulative_line_numbers, cumulative_codes, cumulative_content.rstrip()))\n",
        "        #print('-' * 80)  # Separator for clarity\n",
        "        whole_trajectory.append(window_group[-1])\n",
        "        try:\n",
        "            temp = cumulative_content + list_of_tuples[i + n][2] + \"\\n\" + list_of_tuples[i + n + 1][2]\n",
        "        except:\n",
        "            try:\n",
        "                temp = cumulative_content + list_of_tuples[i + n][2]\n",
        "            except:\n",
        "                temp = cumulative_content\n",
        "        if (i-2 >= 0):\n",
        "            temp = list_of_tuples[i - 2][2] + \"\\n\" + list_of_tuples[i - 1][2] + \"\\n\" + temp\n",
        "        elif (i-1 >= 0):\n",
        "            temp = list_of_tuples[i - 1][2] + \"\\n\" + temp\n",
        "        extra_buffer_for_whole_trajectory.append(temp)\n",
        "    return whole_trajectory, extra_buffer_for_whole_trajectory"
      ],
      "metadata": {
        "id": "F8edWyeZSZzf"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import cmp_to_key\n",
        "import re\n",
        "\n",
        "\n",
        "def check_if_string_is_contained_by_a_dict_item(string, dict_of_items):\n",
        "    for key, val in dict_of_items.items():\n",
        "        if string in key[2]:\n",
        "            return True, val\n",
        "    return False, None\n",
        "\n",
        "\n",
        "def check_if_string_contains_a_dict_item(string, dict_of_items):\n",
        "    for key, val in dict_of_items.items():\n",
        "        if key[2] in string:\n",
        "            del dict_of_items[key]  # remove since it needs to be replaced by the larger entry, in the main func\n",
        "            return True, val\n",
        "    return False, None\n",
        "\n",
        "\n",
        "def handle_potential_overlap_between_candidate_and_pool(candidate_sentence, line_number, code, pool_dict, index):\n",
        "    candidate_sentence_is_contained_by_an_item_of_pool_dict, potential_index = check_if_string_is_contained_by_a_dict_item(candidate_sentence, pool_dict)\n",
        "    if not candidate_sentence_is_contained_by_an_item_of_pool_dict:\n",
        "        candidate_sentence_contains_an_item_of_pool_dict, potential_index = check_if_string_contains_a_dict_item(candidate_sentence, pool_dict)\n",
        "        if candidate_sentence_contains_an_item_of_pool_dict:\n",
        "            pool_dict[(line_number, code, candidate_sentence)] = potential_index\n",
        "            #print(\"candidate_sentence_contains_an_item_of_pool_dict\")\n",
        "        else:\n",
        "            pool_dict[(line_number, code, candidate_sentence)] = index\n",
        "            index += 1\n",
        "            #print(\"candidate_sentence totally unique; adding it\")\n",
        "    else:\n",
        "        pass\n",
        "        #print(\"candidate_sentence_is_contained_by_an_item_of_pool_dict\")\n",
        "    return index\n",
        "\n",
        "\n",
        "def combine_sentence_lists(list1, list2, list1_line_numbers, list1_codes, list2_line_numbers, list2_codes):\n",
        "    combined_sentences_dict = dict()\n",
        "    index = 0\n",
        "    for i, s1 in enumerate(list1):\n",
        "          for j, s2 in enumerate(list2):\n",
        "              if (s1 in s2) and (s2 not in s1):  #we choose s2\n",
        "                  index = handle_potential_overlap_between_candidate_and_pool(s2, list2_line_numbers[j], list2_codes[j], combined_sentences_dict, index)\n",
        "                  continue\n",
        "              elif (s2 in s1) and (s1 not in s2):  #we choose s1\n",
        "                  index = handle_potential_overlap_between_candidate_and_pool(s1, list1_line_numbers[i], list1_codes[i], combined_sentences_dict, index)\n",
        "                  continue\n",
        "              elif (s2 == s1):  #we choose s1\n",
        "                  index = handle_potential_overlap_between_candidate_and_pool(s1, list1_line_numbers[i], list1_codes[i], combined_sentences_dict, index)\n",
        "                  continue\n",
        "              index = handle_potential_overlap_between_candidate_and_pool(s1, list1_line_numbers[i], list1_codes[i], combined_sentences_dict, index)\n",
        "              index = handle_potential_overlap_between_candidate_and_pool(s2, list2_line_numbers[j], list2_codes[j], combined_sentences_dict, index)\n",
        "    return combined_sentences_dict\n",
        "\n",
        "\n",
        "def determine_sentence_order(essay, sentence_container, sentence_prime_container):\n",
        "    sentence = sentence_container[2]\n",
        "    sentence_prime = sentence_prime_container[2]\n",
        "    essay_c = super_clean_text(essay)\n",
        "    sentence_c = super_clean_text(sentence[:-2])  # burn the last 2 chars\n",
        "    sentence_prime_c = super_clean_text(sentence_prime[:-2])  # burn the last 2 chars\n",
        "    # Find the starting index of sentence and sentence_prime in the essay\n",
        "    index_sentence = essay_c.find(sentence_c)\n",
        "    index_sentence_prime = essay_c.find(sentence_prime_c)\n",
        "\n",
        "    # Check which sentence comes first\n",
        "    if index_sentence < index_sentence_prime:\n",
        "        return -1  # sentence comes before sentence_prime\n",
        "    elif index_sentence > index_sentence_prime:\n",
        "        return 1  # sentence_prime comes before sentence\n",
        "    else:\n",
        "        return 0  # Both sentences appear at the same position (unlikely)\n",
        "\n",
        "def sort_sentences_by_order(sentence_list, essay):\n",
        "    # Use cmp_to_key to convert the comparison function to a key function\n",
        "    sorted_sentence_list = sorted(sentence_list, key=cmp_to_key(lambda s1, s2: determine_sentence_order(essay, s1, s2)))\n",
        "    return sorted_sentence_list\n",
        "\n",
        "\n",
        "def grab_col_from_list_of_tuples(list_of_tuples, col_number):\n",
        "    return [tup[col_number] for tup in list_of_tuples]\n"
      ],
      "metadata": {
        "id": "v-nxPnOqUnp3"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/collated-unredacted.txt'\n",
        "list_of_tuples = parse_text_file(file_path)\n",
        "for item in list_of_tuples[:5]:  # Print the first 5 tuples for verification\n",
        "    print(item)\n",
        "\n",
        "whole_trajectory2, extra_buffer_for_whole_trajectory = sliding_window(list_of_tuples, 4)"
      ],
      "metadata": {
        "id": "Q8XYlzOiS-IO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_merged = []\n",
        "\n",
        "prev_line_numbers, prev_codes, prev_content = whole_trajectory2[0]\n",
        "prev_extra_buffer = extra_buffer_for_whole_trajectory[0]\n",
        "\n",
        "line_numbers, codes, content = whole_trajectory2[1]\n",
        "extra_buffer = extra_buffer_for_whole_trajectory[1]\n",
        "\n",
        "prev_as_sentences, prev_line_numbers_each_sentence, prev_codes_each_sentence = split_text_into_sentences(prev_content, prev_line_numbers, prev_codes)\n",
        "as_sentences, line_numbers_each_sentence, codes_each_sentence = split_text_into_sentences(content, line_numbers, codes)\n",
        "\n",
        "temp_dict = combine_sentence_lists(prev_as_sentences, as_sentences, prev_line_numbers_each_sentence, prev_codes_each_sentence, line_numbers_each_sentence, codes_each_sentence)\n",
        "temp_merged = list(temp_dict.keys())\n",
        "temp_merged_sorted = sort_sentences_by_order(temp_merged, extra_buffer)\n",
        "full_merged += list(temp_merged_sorted)\n",
        "\n",
        "for i, window in enumerate(whole_trajectory2[2:], start=2):\n",
        "    line_numbers, codes, content = window\n",
        "    extra_buffer = extra_buffer_for_whole_trajectory[i]\n",
        "\n",
        "    nextchunk_as_sentences, nextchunk_line_numbers, nextchunk_codes = split_text_into_sentences(content, line_numbers, codes)\n",
        "\n",
        "    snorf_amount = min(len(nextchunk_as_sentences), len(full_merged))\n",
        "    input1 = full_merged[(-1*snorf_amount):]       #aliquot the last chunk\n",
        "    full_merged = full_merged[:(-1*snorf_amount)]  #yes really remove it\n",
        "\n",
        "    input1_line_numbers = grab_col_from_list_of_tuples(input1, 0)\n",
        "    input1_codes = grab_col_from_list_of_tuples(input1, 1)\n",
        "    input1_as_sentences = grab_col_from_list_of_tuples(input1, 2)\n",
        "\n",
        "    temp_dict = combine_sentence_lists(input1_as_sentences, nextchunk_as_sentences, input1_line_numbers, input1_codes, nextchunk_line_numbers, nextchunk_codes)\n",
        "    temp_merged = list(temp_dict.keys())\n",
        "    temp_merged_sorted = sort_sentences_by_order(temp_merged, extra_buffer)\n",
        "    full_merged += list(temp_merged_sorted)\n"
      ],
      "metadata": {
        "id": "mgA3a7Iu6kX4"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for s in full_merged:\n",
        "    print(s)\n"
      ],
      "metadata": {
        "id": "demjfYW5vJP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime, timezone\n",
        "\n",
        "def collect_all_timestamp_sentences(sentenceTuples_list):\n",
        "    timestamp_sentences = []\n",
        "    for s in sentenceTuples_list:\n",
        "        if re.match(r'^\\[\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2} UTC\\]', s[2]):\n",
        "            timestamp_sentences.append(s)\n",
        "    # sort by first item of tuples (the sentence index)\n",
        "    timestamp_sentences = sorted(timestamp_sentences, key=lambda x: x[0])\n",
        "\n",
        "    # parse timestamps and add actual timestamp object with UTC timezone to each tuple\n",
        "    timestamp_sentences_parsed = []\n",
        "    for ts_tuple in timestamp_sentences:\n",
        "        s = ts_tuple[2]\n",
        "        # typical value of s: '[2024-08-19 00:03:56 UTC] (User):'\n",
        "        # grab only what is inside the brackets using a regex\n",
        "        timestamp_string = re.search(r'(\\[.*?\\])', s).group(1)\n",
        "        timestamp_object = datetime.strptime(timestamp_string, '[%Y-%m-%d %H:%M:%S UTC]')\n",
        "        timestamp_object = timestamp_object.replace(tzinfo=timezone.utc)\n",
        "        new_tuple = (ts_tuple[0], ts_tuple[1], ts_tuple[2], timestamp_object)\n",
        "        timestamp_sentences_parsed.append(new_tuple)\n",
        "    return timestamp_sentences_parsed\n",
        "\n",
        "all_timestamp_sentences = collect_all_timestamp_sentences(full_merged)\n"
      ],
      "metadata": {
        "id": "YEh7y2x6D-Qs"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for s in all_timestamp_sentences:\n",
        "    print(s)"
      ],
      "metadata": {
        "id": "YITO_WNYD-OY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_merged[0]"
      ],
      "metadata": {
        "id": "FxiHBlBOCEnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())  # Should return True\n",
        "print(torch.cuda.current_device())  # Should return 0 (the index of the GPU)\n",
        "print(torch.cuda.get_device_name(0))  # Should return \"Tesla T4\"\n"
      ],
      "metadata": {
        "id": "rTebSi5sI_ro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "id": "dCA3Jt5aQRto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "model = SentenceTransformer('all-mpnet-base-v2', device='cuda')\n",
        "sentences = [\"This is a test sentence\"] * 1000\n",
        "\n",
        "start_time = time.time()\n",
        "embeddings = model.encode(sentences, convert_to_tensor=True)\n",
        "end_time = time.time()\n",
        "\n",
        "print(f\"Time taken: {end_time - start_time} seconds\")"
      ],
      "metadata": {
        "id": "TFwhiaIpJvbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_similarity(embedding1, embedding2):\n",
        "    \"\"\"Compute cosine similarity between two embeddings.\"\"\"\n",
        "    return util.pytorch_cos_sim(embedding1.to('cuda'), embedding2.to('cuda')).item()\n",
        "\n",
        "\n",
        "def get_best_hit(query_sentence, subject_pool):\n",
        "    best_hit = (None, -1.0)\n",
        "    query_sentence_embedding, _ = get_message_embedding(query_sentence)\n",
        "    total = len(subject_pool)\n",
        "    i = 0\n",
        "    for candidate_sentence_tuple in subject_pool:\n",
        "        candidate_sentence = candidate_sentence_tuple[2]\n",
        "        candidate_embedding, _ = get_message_embedding(candidate_sentence)\n",
        "        similarity = compute_similarity(query_sentence_embedding, candidate_embedding)\n",
        "        if similarity > best_hit[1]:\n",
        "            best_hit = (candidate_sentence_tuple, similarity)\n",
        "            #print(f\"{(100*i/total)}% | best hit so far: {best_hit}\")\n",
        "        i += 1\n",
        "    #print(f\"final best hit: {best_hit}\")\n",
        "    return best_hit\n",
        "\n",
        "\n",
        "SENTENCE_TIMESTAMPS = {}\n",
        "\n",
        "\n",
        "def get_timestamp_just_before(index_of_sentence_tuple, ts_sentences_tuple_list):\n",
        "    for i, ts_sentence_tuple in enumerate(ts_sentences_tuple_list):\n",
        "        if ts_sentence_tuple[0] > index_of_sentence_tuple:\n",
        "            return ts_sentences_tuple_list[i-1]\n",
        "    return None"
      ],
      "metadata": {
        "id": "vDnoXXReOUSl"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for s in full_merged:\n",
        "    index_of_sentence_tuple = s[0]\n",
        "    timestamp_just_before = get_timestamp_just_before(index_of_sentence_tuple, all_timestamp_sentences)\n",
        "    SENTENCE_TIMESTAMPS[s] = timestamp_just_before"
      ],
      "metadata": {
        "id": "LZ0uhlpE-1qk"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "import hashlib\n",
        "\n",
        "model = SentenceTransformer('all-mpnet-base-v2', device='cuda')\n",
        "SIMILARITY_THRESHOLD = 0.15\n",
        "EMBEDDING_TRACKER_DICT = {}\n",
        "\n",
        "def get_message_embedding(message_text):\n",
        "    \"\"\"Generate an SBERT embedding and SHA-256 hash for the given text.\"\"\"\n",
        "    sha256_hash = hashlib.sha256(message_text.encode('utf-8')).hexdigest()\n",
        "    if sha256_hash in EMBEDDING_TRACKER_DICT:\n",
        "        return EMBEDDING_TRACKER_DICT[sha256_hash], sha256_hash\n",
        "    embedding = model.encode(message_text, convert_to_tensor=True, device='cuda')\n",
        "    EMBEDDING_TRACKER_DICT[sha256_hash] = embedding\n",
        "    return embedding, sha256_hash\n",
        "\n",
        "def get_sentences_before(ts, source_pool, ts_map_dict):\n",
        "    filtered_pool = []\n",
        "    for candidate_tuple in source_pool:\n",
        "        candidate = ts_map_dict[candidate_tuple]\n",
        "        if candidate is None:\n",
        "            continue\n",
        "        candidate_ts = candidate[3]\n",
        "        if candidate_ts < ts:\n",
        "            filtered_pool.append(candidate_tuple)\n",
        "    return filtered_pool"
      ],
      "metadata": {
        "id": "mCc1kyL-QRlK"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "for sentence_tuple in full_merged:\n",
        "    sentence = sentence_tuple[2]\n",
        "    sentence_embedding, sentence_hash = get_message_embedding(sentence)\n",
        "    EMBEDDING_TRACKER_DICT[sentence_hash] = sentence_embedding\n",
        "    print(sentence_tuple[0])\n",
        "\n",
        "\n",
        "with open('EMBEDDING_TRACKER_DICT.pickle', 'wb') as handle:\n",
        "    # Move embeddings to CPU before pickling\n",
        "    EMBEDDING_TRACKER_DICT_CPU = {k: v.cpu() for k, v in EMBEDDING_TRACKER_DICT.items()}\n",
        "    pickle.dump(EMBEDDING_TRACKER_DICT_CPU, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
      ],
      "metadata": {
        "id": "6S9nGucF-5SD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for reloading the pickle file\n",
        "with open('EMBEDDING_TRACKER_DICT.pickle', 'rb') as handle:\n",
        "    EMBEDDING_TRACKER_DICT = pickle.load(handle)\n",
        "    # Move back to CUDA\n",
        "    EMBEDDING_TRACKER_DICT = {k: v.to('cuda') for k, v in EMBEDDING_TRACKER_DICT.items()}\n"
      ],
      "metadata": {
        "id": "8E8M_K1eHTSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ancestry(query_string, subject_pool, ts_map_dict, outfile):\n",
        "    best = get_best_hit(query_string, subject_pool)\n",
        "    ts_of_best = ts_map_dict[best[0]][3] if best[0] else None\n",
        "    if ts_of_best:\n",
        "        ts = ts_of_best.strftime('%Y-%m-%d %H:%M:%S UTC')\n",
        "        line_number = best[0][0]\n",
        "        line_number_constant_width = str(line_number).rjust(6)\n",
        "        line_category = best[0][1]\n",
        "        content = best[0][2]\n",
        "        similarity = best[1]\n",
        "        formatted_line = f\"{ts}: {line_number_constant_width} {line_category} {similarity:.8f} {content}\"\n",
        "        print(formatted_line)\n",
        "        outfile.write(formatted_line + '\\n')  # Write to file with a newline character\n",
        "    similarity = best[1]\n",
        "    if best[0]:\n",
        "        q2 = best[0][2]\n",
        "        ts_of_best = ts_map_dict[best[0]][3]\n",
        "        pool1 = get_sentences_before(ts_of_best, subject_pool, ts_map_dict)\n",
        "        if similarity > 0.1 and len(subject_pool) > 0:\n",
        "            get_ancestry(q2, pool1, ts_map_dict, outfile)\n",
        "        #else:\n",
        "        #   return best\n",
        ""
      ],
      "metadata": {
        "id": "mUipb-nKZ0KI"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"My spoon is too big.\"\n",
        "current_datetime_utc = datetime.now(timezone.utc)\n",
        "ts = current_datetime_utc.strftime('%Y-%m-%d %H:%M:%S UTC')\n",
        "print(f\"{ts}:                       {query}\")\n",
        "result = get_ancestry(query, full_merged, SENTENCE_TIMESTAMPS)\n",
        "#print(result)"
      ],
      "metadata": {
        "id": "agQhjcgTghTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install odfpy\n",
        "!sudo apt-get install pandoc"
      ],
      "metadata": {
        "id": "CpRrjRG4K61N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import os\n",
        "import re\n",
        "\n",
        "\n",
        "def convert_odt_to_txt(odt_file_path, txt_file_path):\n",
        "    # Run the pandoc command to convert the ODT file to TXT\n",
        "    command = ['pandoc', '-s', odt_file_path, '-o', txt_file_path]\n",
        "    subprocess.run(command, check=True)\n",
        "\n",
        "    # Read the contents of the newly created TXT file\n",
        "    with open(txt_file_path, 'r', encoding='utf-8') as txt_file:\n",
        "        text_content = txt_file.read()\n",
        "\n",
        "    return text_content\n",
        "\n",
        "\n",
        "def split_text_into_sentences(text):\n",
        "    original_text = text\n",
        "    # Remove extra tabs\n",
        "    text = re.sub(r'\\t+', ' ', text)\n",
        "\n",
        "    # Remove all asterisks and pound signs\n",
        "    text = re.sub(r'[\\*#]', '', text)\n",
        "\n",
        "    # Replace multiple newlines with a placeholder to identify paragraphs\n",
        "    text = re.sub(r'\\n{2,}', r'###PARAGRAPH###', text)\n",
        "\n",
        "    # Replace single newlines with spaces\n",
        "    text = re.sub(r'\\n+', ' ', text)\n",
        "\n",
        "    # Remove consecutive spaces\n",
        "    text = re.sub(r'\\s{2,}', ' ', text)\n",
        "\n",
        "    # Special handling for \"v.\" in legal citations and other exceptions\n",
        "    text = re.sub(r'\\b(v\\.)\\s+', r'\\1###NO_SPLIT###', text)\n",
        "\n",
        "    # Protect abbreviations, titles, section names, numbering like \"1\\.\", and Roman numerals like \"II.\", \"Pp.\"\n",
        "    text = re.sub(r'\\b(Dr|Mr|Ms|Mrs|U\\.S|Jr|Sr|Cf|cf|art|Art|Pp|pp|[0-9]+|I{1,3}|IV|V|VI|VII|VIII|IX|X|XI|XII|Ibid)\\.\\s+', r'\\1.###NO_SPLIT###', text)\n",
        "\n",
        "    # Protect single-letter section names like \"A.\", \"B.\"\n",
        "    text = re.sub(r'\\b([A-Z])\\.\\s+', r'\\1###NO_SPLIT###', text)\n",
        "\n",
        "    # Protect section numbers like \"(1)\", \"(2)\", \"(a)\"\n",
        "    text = re.sub(r'\\((\\d+|[a-zA-Z])\\)\\s+', r'(\\1)###NO_SPLIT###', text)\n",
        "\n",
        "    def split_after_pattern(text, pattern):\n",
        "        # Use re.split but include the pattern in the split\n",
        "        parts = re.split(pattern, text)\n",
        "\n",
        "        # Reassemble the parts to ensure each matched pattern is followed by its text\n",
        "        sentences = []\n",
        "        for i in range(1, len(parts)):  # Iterate over pattern matches\n",
        "            sentences.append(parts[i].strip())\n",
        "\n",
        "        if len(parts)==1:\n",
        "            sentences = [text]\n",
        "        #elif len(parts) > 1:\n",
        "        #    print(f\"[split_after_pattern] sentences: {sentences}\")\n",
        "\n",
        "        return sentences\n",
        "\n",
        "    # Adjusted regex pattern to handle sentence splitting, especially around footnotes\n",
        "    sentence_endings = re.compile(\n",
        "        r'(?<!###NO_SPLIT###)\\.\\s+(?=[A-Z])|'   # Split at normal sentence boundaries\n",
        "        r'(?<=\\.\\\")\\s+(?=[A-Z])|'               # Period followed by quote and space\n",
        "        r'(?<=[!?])\\s+(?=[A-Z])|'              # Exclamation or question mark followed by space\n",
        "        r'(?<=\\))\\s+(?=[A-Z])|'                 # Closing parenthesis followed by space\n",
        "        r'(?<=\\])\\s+(?=[A-Z])|'                 # Footnote marker followed by space, but only if followed by an uppercase letter (new sentence)\n",
        "        r'(?<=\\.\\s)(?=\\(?[A-Z])|'              # Handling for \"(iv)\", \"(ii)\", etc.\n",
        "        r'(?<=\\.\\s)(?=[A-Z]\\w+ v\\.\\s[A-Z])|'   # Handling for \"Nixon v. Fitzgerald\"\n",
        "        r'(?=\\[\\d{4}-\\d{2}-\\d{2})|'             # Split before a timestamp pattern \"[2024-07-23...\"\n",
        "        r'(?<=\\.\\s)(?=Pp\\.\\s[0-9]+)'           # Handling for \"Pp. 24-28\"\n",
        "    )\n",
        "\n",
        "    # Split the text into sentences\n",
        "    sentences = sentence_endings.split(text.strip())\n",
        "\n",
        "    # Apply split_after_pattern to each sentence and flatten the result\n",
        "    # Pattern to match the entire timestamp, descriptor, and model line\n",
        "    # split after timestamp metadata of assistant or tool message\n",
        "    pattern = r'(\\[\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2} UTC\\] \\([A-Za-z]+\\) - Model: [^\\:]{5,6}: )'\n",
        "    temp_sentences = []\n",
        "    for s in sentences:\n",
        "        temp_sentences.extend(split_after_pattern(s, pattern))\n",
        "    sentences = temp_sentences\n",
        "\n",
        "    # split after timestamp metadata of user message\n",
        "    pattern = r'(\\[\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2} UTC\\] \\(User\\): )'\n",
        "    temp_sentences = []\n",
        "    for s in sentences:\n",
        "        temp_sentences.extend(split_after_pattern(s, pattern))\n",
        "    sentences = temp_sentences\n",
        "\n",
        "    # Replace placeholder back to original\n",
        "    sentences = [s.replace(\"###NO_SPLIT###\", \" \") for s in sentences]\n",
        "\n",
        "    # Correctly merge footnote markers back into the sentence when inside quotes or before dashes\n",
        "    merged_sentences = []\n",
        "    temp_sentence = \"\"\n",
        "    for sentence in sentences:\n",
        "        if re.match(r'^\\[\\^\\d+\\]$', sentence):\n",
        "            temp_sentence += sentence  # Append footnote marker to previous sentence\n",
        "        else:\n",
        "            if temp_sentence:\n",
        "                merged_sentences.append(temp_sentence.strip())\n",
        "                temp_sentence = \"\"\n",
        "            merged_sentences.append(sentence.strip())\n",
        "\n",
        "    # Ensure any remaining temp_sentence is added\n",
        "    if temp_sentence:\n",
        "        merged_sentences.append(temp_sentence.strip())\n",
        "\n",
        "    # Split based on paragraph boundaries\n",
        "    final_sentences_with_paragraphs = []\n",
        "    for sentence in merged_sentences:\n",
        "        final_sentences_with_paragraphs.extend(sentence.split('###PARAGRAPH###'))\n",
        "\n",
        "    # Remove the unnecessary colons\n",
        "    final_sentences_with_paragraphs = [re.sub(r'^: ', '', s).strip() for s in final_sentences_with_paragraphs]\n",
        "\n",
        "    # Return the cleaned list of sentences\n",
        "    cleaned_sentences = [sentence.strip() for sentence in final_sentences_with_paragraphs if sentence.strip()]\n",
        "\n",
        "    return cleaned_sentences #, line_number_for_each_sentence, code_for_each_sentence\n",
        "\n",
        "\n",
        "\n",
        "odt_file_path = \"/content/PerfectLegitimateBribe-anonymized.odt\"\n",
        "txt_file_path = \"/content/PerfectLegitimateBribe-anonymized.txt\"\n",
        "\n",
        "# Convert the ODT file to a TXT file and read the content\n",
        "extracted_text = convert_odt_to_txt(odt_file_path, txt_file_path)\n",
        "\n",
        "sentences_list = split_text_into_sentences(extracted_text)\n",
        "\n",
        "\n",
        "for i, sentence in enumerate(sentences_list):\n",
        "    print(f'sentence #{i}: {sentence}')"
      ],
      "metadata": {
        "id": "oyHhVZ2tJl_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "current_datetime_utc = datetime.now(timezone.utc)\n",
        "ts = current_datetime_utc.strftime('%Y-%m-%d %H:%M:%S UTC')\n",
        "with open('ancestry-unredacted.txt', 'a') as file:\n",
        "    for i, sentence in enumerate(sentences_list):\n",
        "        i_constant_width = str(i).rjust(4)\n",
        "        formatted_line = f\"{ts}:    sentence #{i_constant_width}:    {sentence}\"\n",
        "        print(formatted_line)\n",
        "        file.write(formatted_line + '\\n')  # Write to file with a newline character\n",
        "\n",
        "        result = get_ancestry(sentence, full_merged, SENTENCE_TIMESTAMPS, file)\n",
        "\n",
        "        print(\"------\")\n",
        "        file.write(\"------\\n\")  # Write the separator to the file"
      ],
      "metadata": {
        "id": "EQ-oXaoS-buO"
      },
      "execution_count": 119,
      "outputs": []
    }
  ]
}