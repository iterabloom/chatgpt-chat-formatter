{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "id": "JOc7Z1OkDz_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3YRbd78rNX1"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from datetime import datetime\n",
        "import textwrap\n",
        "import re\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import hashlib\n",
        "import os\n",
        "import glob\n",
        "\n",
        "model = SentenceTransformer('all-mpnet-base-v2')\n",
        "SIMILARITY_THRESHOLD = 0.15\n",
        "EMBEDDING_TRACKER_DICT = {}\n",
        "ALL_RELEVANT_MEMORIES = {}\n",
        "GLOBAL_LINES = []  # Global list to store lines with their types\n",
        "\n",
        "\n",
        "def format_timestamp(timestamp):\n",
        "    \"\"\"Convert a timestamp to a human-readable date and time format.\"\"\"\n",
        "    return datetime.utcfromtimestamp(timestamp).strftime('%Y-%m-%d %H:%M:%S UTC')\n",
        "\n",
        "\n",
        "def is_only_whitespace(s):\n",
        "    # Regex pattern to match only whitespace characters (spaces, tabs, newlines, etc.)\n",
        "    pattern = r'^\\s*$'\n",
        "\n",
        "    # Test if the string matches the pattern\n",
        "    return bool(re.match(pattern, s))\n",
        "\n",
        "\n",
        "def pretty_print_message(message, indent=0, width=80, cutoff_date=None, prev_message=None):\n",
        "    if indent+4 > width:\n",
        "        raise ValueError(\"Indent cannot exceed the width.\")\n",
        "\n",
        "    line_types = []\n",
        "    if message:\n",
        "        \"\"\"Pretty print a single message with text wrapping and newline preservation.\"\"\"\n",
        "        author_role = message.get('author', {}).get('role', 'unknown')\n",
        "        content_parts = message.get('content', {}).get('parts', [])\n",
        "        content_searchText = message.get('content', {}).get('text', \"\")\n",
        "        content_browsingResult = message.get('content', {}).get('result', \"\")\n",
        "        content_memories = message.get('content', {}).get('model_set_context', None)\n",
        "        model_slug = message.get('metadata', {}).get('model_slug', None)\n",
        "        recipient = message.get('recipient', \"\")  #will be \"bio\" for new memory creation\n",
        "\n",
        "        if prev_message:\n",
        "            prev_message_content_parts = prev_message.get('content', {}).get('parts', [])\n",
        "        else:\n",
        "            prev_message_content_parts = []\n",
        "\n",
        "        # Skip empty system messages\n",
        "        if author_role == 'system' and not any(content_parts):\n",
        "            return\n",
        "\n",
        "        timestamp = message.get('create_time')\n",
        "\n",
        "        # Determine the prefix code based on the author role\n",
        "        if author_role == 'user' or len(content_browsingResult) > 0 or content_memories:\n",
        "            line_code = '[P]'\n",
        "        elif author_role == 'assistant' and not content_searchText and (recipient==\"all\"):\n",
        "            line_code = '[C]'\n",
        "        else:\n",
        "            line_code = '[N]'\n",
        "\n",
        "        # Prepare the prefix (timestamp, author role, and model slug)\n",
        "        if timestamp:\n",
        "            prefix = f\"[{format_timestamp(timestamp)}] ({author_role.capitalize()})\"\n",
        "        else:\n",
        "            prefix = f\"({author_role.capitalize()})\"\n",
        "\n",
        "        if model_slug:\n",
        "            prefix += f\" - Model: {model_slug}\"\n",
        "\n",
        "        prefix += ':'\n",
        "\n",
        "        # Add the prefix line to the global list\n",
        "        GLOBAL_LINES.append(('[N]', ' ' * indent + prefix))\n",
        "\n",
        "        # Wrap and append the message content, respecting newlines\n",
        "        for part in content_parts:\n",
        "            if isinstance(part, list):\n",
        "                for paragraph in part.splitlines():\n",
        "                    if is_only_whitespace(paragraph):\n",
        "                        paragraph = \"\"\n",
        "                    wrapped_text = textwrap.fill(paragraph, width=width,\n",
        "                                                initial_indent=' ' * (indent + 4),\n",
        "                                                subsequent_indent=' ' * (indent + 4))\n",
        "                    for line in wrapped_text.splitlines():\n",
        "                        GLOBAL_LINES.append((line_code, line))\n",
        "            else:\n",
        "                wrapped_text = textwrap.fill(str(part), width=width,\n",
        "                                            initial_indent=' ' * (indent + 4),\n",
        "                                            subsequent_indent=' ' * (indent + 4))\n",
        "                for line in wrapped_text.splitlines():\n",
        "                    GLOBAL_LINES.append((line_code, line))\n",
        "\n",
        "        if content_memories and cutoff_date:\n",
        "            filtered_memories = split_and_filter_memories(content_memories, cutoff_date)\n",
        "            subject_message = \" \".join(prev_message_content_parts).strip()\n",
        "            relevant_memories = filter_memories_by_similarity(subject_message, filtered_memories, threshold=SIMILARITY_THRESHOLD)\n",
        "            if relevant_memories:\n",
        "                memory_info = f\"Memories with similarity above {SIMILARITY_THRESHOLD} to last message:\"\n",
        "            else:\n",
        "                memory_info = f\"[Note from collator.ipynb - No memories found with similarity above {SIMILARITY_THRESHOLD} and more recent than {cutoff_date}.]\"\n",
        "            wrapped_text = textwrap.fill(memory_info, width=width,\n",
        "                                          initial_indent=' ' * (indent + 4),\n",
        "                                          subsequent_indent=' ' * (indent + 4))\n",
        "            for line in wrapped_text.splitlines():\n",
        "                GLOBAL_LINES.append(('[N]', line))\n",
        "            for memory in relevant_memories:\n",
        "                ALL_RELEVANT_MEMORIES[memory] = True\n",
        "                wrapped_text = textwrap.fill(memory, width=width,\n",
        "                                            initial_indent=' ' * (indent + 4),\n",
        "                                            subsequent_indent=' ' * (indent + 4))\n",
        "                for line in wrapped_text.splitlines():\n",
        "                    GLOBAL_LINES.append((line_code, line))\n",
        "        if content_searchText:\n",
        "            wrapped_text = textwrap.fill(content_searchText, width=width,\n",
        "                                        initial_indent=' ' * (indent + 4),\n",
        "                                        subsequent_indent=' ' * (indent + 4))\n",
        "            for line in wrapped_text.splitlines():\n",
        "                GLOBAL_LINES.append((line_code, line))\n",
        "        if content_browsingResult:\n",
        "            wrapped_text = textwrap.fill(content_browsingResult, width=width,\n",
        "                                        initial_indent=' ' * (indent + 4),\n",
        "                                        subsequent_indent=' ' * (indent + 4))\n",
        "            for line in wrapped_text.splitlines():\n",
        "                GLOBAL_LINES.append((line_code, line))\n",
        "\n",
        "\n",
        "def split_and_filter_memories(memories, cutoff_date):\n",
        "    \"\"\"Split the memories string into individual entries and filter them by cutoff_date.\"\"\"\n",
        "    memory_entries = re.split(r'\\n(?=\\d{2}\\. \\[\\d{4}-\\d{2}-\\d{2}\\])', memories)\n",
        "\n",
        "    filtered_memories = []\n",
        "\n",
        "    for entry in memory_entries:\n",
        "        match = re.search(r'\\[(\\d{4}-\\d{2}-\\d{2})\\]', entry)\n",
        "        if match:\n",
        "            entry_date_str = match.group(1)\n",
        "            entry_date = datetime.strptime(entry_date_str, '%Y-%m-%d')\n",
        "            if entry_date >= cutoff_date:\n",
        "                filtered_memories.append(entry)\n",
        "\n",
        "    return filtered_memories\n",
        "\n",
        "\n",
        "def compute_similarity(embedding1, embedding2):\n",
        "    \"\"\"Compute cosine similarity between two embeddings.\"\"\"\n",
        "    return util.pytorch_cos_sim(embedding1, embedding2).item()\n",
        "\n",
        "\n",
        "def get_message_embedding(message_text):\n",
        "    \"\"\"Generate an SBERT embedding and SHA-256 hash for the given text.\"\"\"\n",
        "    sha256_hash = hashlib.sha256(message_text.encode('utf-8')).hexdigest()\n",
        "    if sha256_hash in EMBEDDING_TRACKER_DICT:\n",
        "        return EMBEDDING_TRACKER_DICT[sha256_hash], sha256_hash\n",
        "    embedding = model.encode(message_text, convert_to_tensor=True)\n",
        "    EMBEDDING_TRACKER_DICT[sha256_hash] = embedding\n",
        "    return embedding, sha256_hash\n",
        "\n",
        "\n",
        "def filter_memories_by_similarity(user_message, memories, threshold=0.7):\n",
        "    \"\"\"Filter memories based on their similarity to the user message.\"\"\"\n",
        "    user_embedding, user_hash = get_message_embedding(user_message)\n",
        "    EMBEDDING_TRACKER_DICT[user_hash] = user_embedding\n",
        "\n",
        "    # Process memories and filter based on similarity\n",
        "    relevant_memories = []\n",
        "    for memory in memories:\n",
        "        memory_embedding, memory_hash = get_message_embedding(memory)\n",
        "        EMBEDDING_TRACKER_DICT[memory_hash] = memory_embedding\n",
        "        similarity = compute_similarity(user_embedding, memory_embedding)\n",
        "        if similarity >= threshold:\n",
        "            relevant_memories.append(memory)\n",
        "\n",
        "    return relevant_memories\n",
        "\n",
        "\n",
        "def traverse_conversation(node_id, mapping, indent=0, width=80, first_call=True, second_call=False, cutoff_date=None, prev_message=None):\n",
        "    \"\"\"Recursively traverse and print the conversation tree with text wrapping.\"\"\"\n",
        "    node = mapping.get(node_id)\n",
        "    first_timestamp = None\n",
        "\n",
        "    if not node:\n",
        "        print(f\"Node with ID {node_id} not found.\")\n",
        "        return None\n",
        "\n",
        "    message = node.get('message')\n",
        "    if message:\n",
        "        pretty_print_message(message, indent, width, cutoff_date, prev_message)\n",
        "        first_timestamp = message.get('create_time')\n",
        "\n",
        "    # Traverse children nodes if any\n",
        "    children = node.get('children', [])\n",
        "    if first_call:  # Do not increase indent for the first call (System message)\n",
        "        for child_id in children:\n",
        "            temp_ts = traverse_conversation(child_id, mapping, indent, width, first_call=False, second_call=True, cutoff_date=cutoff_date, prev_message=message)\n",
        "            if not first_timestamp:\n",
        "                first_timestamp = temp_ts\n",
        "    elif second_call:  # Do not increase indent for the second call (System message)\n",
        "        for child_id in children:\n",
        "            temp_ts = traverse_conversation(child_id, mapping, indent, width, first_call=False, second_call=False, cutoff_date=cutoff_date, prev_message=message)\n",
        "            if not first_timestamp:\n",
        "                first_timestamp = temp_ts\n",
        "    else:  # Increase indent for all subsequent messages\n",
        "        for child_id in children:\n",
        "            temp_ts = traverse_conversation(child_id, mapping, indent + 4, width, first_call=False, second_call=False, cutoff_date=cutoff_date, prev_message=message)\n",
        "            if not first_timestamp:\n",
        "                first_timestamp = temp_ts\n",
        "    return first_timestamp\n",
        "\n",
        "\n",
        "def find_root_node(mapping):\n",
        "    \"\"\"Find the root node of the conversation tree.\"\"\"\n",
        "    child_to_parent = {child_id: parent_id for parent_id, node in mapping.items() for child_id in node.get('children', [])}\n",
        "    root_id = None\n",
        "\n",
        "    for node_id in mapping:\n",
        "        if node_id not in child_to_parent:\n",
        "            root_id = node_id\n",
        "            break\n",
        "\n",
        "    return root_id\n",
        "\n",
        "def parse_chat_transcripts(directory_path, output_file2, width=80, cutoff_date=None):\n",
        "    \"\"\"Parse and pretty print the conversations from all JSON files in the directory.\"\"\"\n",
        "    intermediate_data2 = {}\n",
        "    intermediate_data_linetypes = {}\n",
        "\n",
        "    for file_path in glob.glob(os.path.join(directory_path, '*.json')):\n",
        "        with open(file_path, 'r') as f:\n",
        "            chat_data = json.load(f)\n",
        "\n",
        "        mapping = chat_data[0].get('mapping', {})\n",
        "        root_id = find_root_node(mapping)\n",
        "\n",
        "        print(file_path)\n",
        "        first_timestamp = traverse_conversation(root_id, mapping, width=width, cutoff_date=cutoff_date)\n",
        "        print(first_timestamp)\n",
        "        print()\n",
        "\n",
        "        if GLOBAL_LINES and first_timestamp:\n",
        "            # Store the lines and their timestamp\n",
        "            sha256_hash = hashlib.sha256(\"\".join([line for _, line in GLOBAL_LINES]).encode('utf-8')).hexdigest()\n",
        "            intermediate_data2[(sha256_hash, first_timestamp)] = list(GLOBAL_LINES)  # Store a copy of the list of tuples\n",
        "            GLOBAL_LINES.clear()  # Clear the global list for the next file\n",
        "\n",
        "    # Sort the intermediate data by the timestamp\n",
        "    sorted_intermediate_data2 = sorted(intermediate_data2.items(), key=lambda item: item[0][1])\n",
        "\n",
        "    # Determine the number of digits for the line numbers\n",
        "    total_lines2 = sum(len(lines) for (_, _), lines in sorted_intermediate_data2)\n",
        "    max_line_number_digits2 = len(str(total_lines2))\n",
        "\n",
        "    # Write the sorted pretty-printed messages to the output file with line numbers\n",
        "    with open(output_file2, 'w') as output_f:\n",
        "        current_line = 1\n",
        "        for (hash_key, ts), lines in sorted_intermediate_data2:\n",
        "            for line_code, line in lines:\n",
        "                # Prepend the line number and line type to each line, with spaces for alignment\n",
        "                line_number_str = f\"{current_line:>{max_line_number_digits2}}\"\n",
        "                try:\n",
        "                    output_f.write(f\"{line_number_str} {line_code} | {line}\\n\")\n",
        "                except:\n",
        "                    output_f.write(f\"{line_number_str} X | {line}\\n\")\n",
        "                current_line += 1\n",
        "\n",
        "\n",
        "# Provide the directory path and output file path\n",
        "directory_path = '/content/json_files'\n",
        "output_file2 = '/content/collated-unredacted.txt'\n",
        "cutoff_date_str = '2020-07-01'\n",
        "cutoff_date = datetime.strptime(cutoff_date_str, '%Y-%m-%d')\n",
        "parse_chat_transcripts(directory_path, output_file2, width=340, cutoff_date=cutoff_date)\n",
        "\n",
        "for k, v in ALL_RELEVANT_MEMORIES.items():\n",
        "    print(k)\n",
        "    print(\"--------\")\n"
      ]
    }
  ]
}